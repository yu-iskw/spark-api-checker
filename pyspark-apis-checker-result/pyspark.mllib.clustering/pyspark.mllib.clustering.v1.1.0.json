{"SparkContext": {"instance_methods": {}, "name": "SparkContext", "class_methods": {"_ensure_initialized": {"argspec": ["cls", "instance", "gateway"], "name": "_ensure_initialized"}, "parallelize": {"argspec": ["self", "c", "numSlices"], "name": "parallelize"}, "_dictToJavaMap": {"argspec": ["self", "d"], "name": "_dictToJavaMap"}, "_getJavaStorageLevel": {"argspec": ["self", "storageLevel"], "name": "_getJavaStorageLevel"}, "newAPIHadoopFile": {"argspec": ["self", "path", "inputFormatClass", "keyClass", "valueClass", "keyConverter", "valueConverter", "conf", "batchSize"], "name": "newAPIHadoopFile"}, "_do_init": {"argspec": ["self", "master", "appName", "sparkHome", "pyFiles", "environment", "batchSize", "serializer", "conf"], "name": "_do_init"}, "setLocalProperty": {"argspec": ["self", "key", "value"], "name": "setLocalProperty"}, "wholeTextFiles": {"argspec": ["self", "path", "minPartitions"], "name": "wholeTextFiles"}, "union": {"argspec": ["self", "rdds"], "name": "union"}, "runJob": {"argspec": ["self", "rdd", "partitionFunc", "partitions", "allowLocal"], "name": "runJob"}, "getLocalProperty": {"argspec": ["self", "key"], "name": "getLocalProperty"}, "cancelJobGroup": {"argspec": ["self", "groupId"], "name": "cancelJobGroup"}, "cancelAllJobs": {"argspec": ["self"], "name": "cancelAllJobs"}, "hadoopRDD": {"argspec": ["self", "inputFormatClass", "keyClass", "valueClass", "keyConverter", "valueConverter", "conf", "batchSize"], "name": "hadoopRDD"}, "_checkpointFile": {"argspec": ["self", "name", "input_deserializer"], "name": "_checkpointFile"}, "newAPIHadoopRDD": {"argspec": ["self", "inputFormatClass", "keyClass", "valueClass", "keyConverter", "valueConverter", "conf", "batchSize"], "name": "newAPIHadoopRDD"}, "setCheckpointDir": {"argspec": ["self", "dirName"], "name": "setCheckpointDir"}, "stop": {"argspec": ["self"], "name": "stop"}, "broadcast": {"argspec": ["self", "value"], "name": "broadcast"}, "clearFiles": {"argspec": ["self"], "name": "clearFiles"}, "sequenceFile": {"argspec": ["self", "path", "keyClass", "valueClass", "keyConverter", "valueConverter", "minSplits", "batchSize"], "name": "sequenceFile"}, "sparkUser": {"argspec": ["self"], "name": "sparkUser"}, "hadoopFile": {"argspec": ["self", "path", "inputFormatClass", "keyClass", "valueClass", "keyConverter", "valueConverter", "conf", "batchSize"], "name": "hadoopFile"}, "addFile": {"argspec": ["self", "path"], "name": "addFile"}, "addPyFile": {"argspec": ["self", "path"], "name": "addPyFile"}, "_initialize_context": {"argspec": ["self", "jconf"], "name": "_initialize_context"}, "accumulator": {"argspec": ["self", "value", "accum_param"], "name": "accumulator"}, "pickleFile": {"argspec": ["self", "name", "minPartitions"], "name": "pickleFile"}, "setSystemProperty": {"argspec": ["cls", "key", "value"], "name": "setSystemProperty"}, "textFile": {"argspec": ["self", "name", "minPartitions"], "name": "textFile"}, "setJobGroup": {"argspec": ["self", "groupId", "description", "interruptOnCancel"], "name": "setJobGroup"}}}, "KMeansModel": {"instance_methods": {}, "name": "KMeansModel", "class_methods": {"predict": {"argspec": ["self", "x"], "name": "predict"}}}, "KMeans": {"instance_methods": {}, "name": "KMeans", "class_methods": {"train": {"argspec": ["cls", "data", "k", "maxIterations", "runs", "initializationMode"], "name": "train"}}}, "SparseVector": {"instance_methods": {}, "name": "SparseVector", "class_methods": {"toArray": {"argspec": ["self"], "name": "toArray"}, "squared_distance": {"argspec": ["self", "other"], "name": "squared_distance"}, "dot": {"argspec": ["self", "other"], "name": "dot"}}}}
